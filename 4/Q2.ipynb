{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"lLrhnLpiGsN-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607147199601,"user_tz":-330,"elapsed":4501,"user":{"displayName":"Aniket Chauhan","photoUrl":"","userId":"11893733794973168978"}},"outputId":"b662947b-b439-48f8-bfb7-c99c33c44604"},"source":["import numpy as np\n","from sklearn.cluster import KMeans\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","import warnings\n","import matplotlib.pyplot as plt \n","from scipy.spatial.distance import cdist\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize \n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score\n","from tabulate import tabulate \n","from sklearn.externals import joblib \n","import joblib as job\n","import pickle as pk\n","warnings.filterwarnings(\"ignore\")\n","nltk.download('stopwords')\n","nltk.download('punkt')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"6O3-LoN8HlmD","executionInfo":{"status":"ok","timestamp":1607147370914,"user_tz":-330,"elapsed":1355,"user":{"displayName":"Aniket Chauhan","photoUrl":"","userId":"11893733794973168978"}}},"source":["#2.1.\n","from sklearn.model_selection import train_test_split\n","path = '/content/drive/My Drive/Projects/ML_Assignment/ML_Assignment_4/Dataset/yelp_labelled.txt'\n","r = 0.3\n","\n","def Split(path,r):\n","  fp = open(path)\n","  lines = fp.read()\n","  lines = lines.split('\\n')\n","  data = []\n","  for i in range(len(lines)):\n","    data.append(lines[i].split('\\t'))\n","  Data = pd.DataFrame(data, columns = ['Lines','Label'])  \n","  Data = Data.dropna()\n","  Data['Label'] = pd.to_numeric(Data['Label'])\n","  Data['Lines'] = Data['Lines'].astype(str)\n","  X = Data['Lines']\n","  Y = Data['Label'] \n","  Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=r,stratify=Y,random_state =3) \n","  return Xtrain, Xtest, Ytrain, Ytest\n","\n","Xtrain, Xtest, Ytrain, Ytest = train_test_split =  Split(path,r)  "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"SesmQKzYQB0S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607147373997,"user_tz":-330,"elapsed":1390,"user":{"displayName":"Aniket Chauhan","photoUrl":"","userId":"11893733794973168978"}},"outputId":"b47254d1-1a9b-4ee2-da9b-098a71866e27"},"source":["print('Xtrain shape :',Xtrain.shape)\n","print('Ytrain shape :',Ytrain.shape)\n","print('Xtest shape :',Xtest.shape)\n","print('Ytest shape :',Ytest.shape)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Xtrain shape : (700,)\n","Ytrain shape : (700,)\n","Xtest shape : (300,)\n","Ytest shape : (300,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j0NvXI0F4505","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607147173007,"user_tz":-330,"elapsed":34510,"user":{"displayName":"Aniket Chauhan","photoUrl":"","userId":"11893733794973168978"}},"outputId":"e41383ce-17a2-464e-c0d4-e76363d63c7e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UV2nMqP1K64z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607147377131,"user_tz":-330,"elapsed":1313,"user":{"displayName":"Aniket Chauhan","photoUrl":"","userId":"11893733794973168978"}},"outputId":"b7433d3e-7440-4386-8872-6321e1a8e420"},"source":["#2.2.\n","\n","def RemovePun(Xtrain, Xtest):\n","  punc = '''!'\"\\,<()-[]%^&*{};:>./?@#$_~'''\n","  \n","  for i in range(len(Xtrain)):\n","    s1 = Xtrain.iloc[i]\n","    s2 = \"\"\n","    for j in s1:\n","      if j not in punc:\n","        s2 = s2+j\n","    Xtrain.iloc[i] = s2\n","\n","  for i in range(len(Xtest)):\n","    s1 = Xtest.iloc[i]\n","    s2 = \"\"\n","    for j in s1:\n","      if j not in punc:\n","        s2 = s2+j\n","    Xtest.iloc[i] = s2  \n","  \n","  return Xtrain, Xtest\n","\n","Xtrain, Xtest = RemovePun(Xtrain, Xtest)\n","print('\\nAfter removing Puncutation,  4 lines of Training Set :\\n')\n","for i in Xtrain[3:7]:\n","  print(i.split(' ')) \n","\n","def LowerCaseWords(Xtrain, Xtest):\n","  \n","  for i in range(len(Xtrain)):\n","    Xtrain.iloc[i] = Xtrain.iloc[i].lower()\n"," \n","  for i in range(len(Xtest)):\n","    Xtest.iloc[i] = Xtest.iloc[i].lower()\n","  \n","  return Xtrain, Xtest\n","\n","Xtrain, Xtest = LowerCaseWords(Xtrain, Xtest)\n","print('\\nAfter lower case , 4 lines of Training Set :\\n')\n","for i in Xtrain[3:7]:\n","  print(i.split(' ')) \n","\n","\n","def RemoveStopWords(Xtrain, Xtest):\n","  \n","  stopw = stopwords.words('english')\n","  NewXtrain =[]\n","  NewXtest =[]\n","\n","  for i in range(len(Xtrain)):\n","    temp = []\n","    for j in (word_tokenize(Xtrain.iloc[i])): \n","      if j not in stopw:\n","        temp.append(j)\n","    NewXtrain.append(temp)\n","\n","  for i in range(len(Xtest)):\n","    temp = []\n","    for j in (word_tokenize(Xtest.iloc[i])): \n","      if j not in stopw:\n","        temp.append(j)\n","    NewXtest.append(temp)\n","\n","  return NewXtrain ,NewXtest\n","\n","Xtrain, Xtest =  RemoveStopWords(Xtrain, Xtest)\n","print('\\nAfter removing stopwords , 4 lines of Training Set :\\n')\n","for i in Xtrain[3:7]:\n","  print(i) \n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["\n","After removing Puncutation,  4 lines of Training Set :\n","\n","['first', 'time', 'there', 'and', 'might', 'just', 'be', 'the', 'last']\n","['Strike', '2', 'who', 'wants', 'to', 'be', 'rushed']\n","['They', 'were', 'goldencrispy', 'and', 'delicious']\n","['The', 'servers', 'are', 'not', 'pleasant', 'to', 'deal', 'with', 'and', 'they', 'dont', 'always', 'honor', 'Pizza', 'Hut', 'coupons']\n","\n","After lower case , 4 lines of Training Set :\n","\n","['first', 'time', 'there', 'and', 'might', 'just', 'be', 'the', 'last']\n","['strike', '2', 'who', 'wants', 'to', 'be', 'rushed']\n","['they', 'were', 'goldencrispy', 'and', 'delicious']\n","['the', 'servers', 'are', 'not', 'pleasant', 'to', 'deal', 'with', 'and', 'they', 'dont', 'always', 'honor', 'pizza', 'hut', 'coupons']\n","\n","After removing stopwords , 4 lines of Training Set :\n","\n","['first', 'time', 'might', 'last']\n","['strike', '2', 'wants', 'rushed']\n","['goldencrispy', 'delicious']\n","['servers', 'pleasant', 'deal', 'dont', 'always', 'honor', 'pizza', 'hut', 'coupons']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N_ck43UKZ4hn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607147392436,"user_tz":-330,"elapsed":1554,"user":{"displayName":"Aniket Chauhan","photoUrl":"","userId":"11893733794973168978"}},"outputId":"f1d2ea7b-4225-4f6c-9323-d9f18a86298b"},"source":["#2.3.\n","\n","def FeatureMatrix(Xtrain, Xtest):\n","  Vocabword = []\n","  for i in Xtrain:\n","    Vocabword =  Vocabword + i\n","  Vocabword = set(Vocabword)\n","\n","\n","  NewXtrain = []\n","  for i in range(len(Xtrain)):\n","    temp = []\n","    for j in Vocabword:\n","      temp.append(Xtrain[i].count(j))\n","    NewXtrain.append(temp)\n","\n","  NewXtest = []\n","  for i in range(len(Xtest)):\n","    temp = []\n","    for j in Vocabword:\n","      temp.append(Xtest[i].count(j))\n","    NewXtest.append(temp)\n","\n","  NewXtrain = np.array(NewXtrain)\n","  NewXtest = np.array(NewXtest)\n","\n","  return NewXtrain,NewXtest,Vocabword\n","\n","NewXtrain,NewXtest,Vocabword = FeatureMatrix(Xtrain, Xtest)\n","\n","print('Size of vocabulary :',len(Vocabword))\n","print('Size of Feature Matrix for training data',NewXtrain.shape)\n","print('Size of Feature Matrix for testing data',NewXtest.shape)\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Size of vocabulary : 1574\n","Size of Feature Matrix for training data (700, 1574)\n","Size of Feature Matrix for testing data (300, 1574)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Atoq0Vi6dE3R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607147414960,"user_tz":-330,"elapsed":1338,"user":{"displayName":"Aniket Chauhan","photoUrl":"","userId":"11893733794973168978"}},"outputId":"bd870bf1-f09f-4abc-e12d-1dc17f71488a"},"source":["#2.4.\n","# mnv = MultinomialNB(alpha = 1 ,fit_prior = False)\n","# mnv.fit(NewXtrain, Ytrain)\n","# path = '/content/drive/My Drive/Projects/ML_Assignment/ML_Assignment_4/Saved_Models/2/'\n","# with open(path+'textclassification.pkl', \"wb\") as f: \n","#   pk.dump(mnv, f)\n","# print(mnv)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["MultinomialNB(alpha=1, class_prior=None, fit_prior=False)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WEmazXaadoXp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607147425535,"user_tz":-330,"elapsed":1531,"user":{"displayName":"Aniket Chauhan","photoUrl":"","userId":"11893733794973168978"}},"outputId":"c2e6ea6f-48ab-4806-bb9c-d2549af0bce8"},"source":["#2.5.\n","\n","def getresults(path,NewXtrain, NewXtest, Ytrain, Ytest):\n","  with open(path+'textclassification.pkl', \"rb\") as f: \n","    mnv = pk.load(f)\n","  Ypredtrain = mnv.predict(NewXtrain)\n","  Ypredtest = mnv.predict(NewXtest)\n","  print('Training Accuracy',round(accuracy_score(Ytrain,Ypredtrain)*100,3),'%')\n","  print('Validation Accuracy',round(accuracy_score(Ytest,Ypredtest)*100,3),'%')\n","  print('Mis classified Instances :')\n","\n","  k = 0\n","  i = 0\n","  Ytest = list(Ytest)\n","  Ypredtest = list(Ypredtest)\n","  output = []\n","  while(k<4):\n","    if(Ytest[i] != Ypredtest[i]):\n","      k = k+1\n","      output.append([Xtest[i],Ytest[i],Ypredtest[i]])\n","    i = i+1  \n","\n","  Output = pd.DataFrame(output,columns=['Line','Actual Label','Predicted Label'])\n","  print(tabulate(Output, tablefmt = 'psql',headers = 'keys'))  \n","\n","path = '/content/drive/My Drive/Projects/ML_Assignment/ML_Assignment_4/Saved_Models/2/'\n","getresults(path,NewXtrain, NewXtest, Ytrain, Ytest)  "],"execution_count":20,"outputs":[{"output_type":"stream","text":["Training Accuracy 96.571 %\n","Validation Accuracy 81.0 %\n","Mis classified Instances :\n","+----+-------------------------------------------------------------+----------------+-------------------+\n","|    | Line                                                        |   Actual Label |   Predicted Label |\n","|----+-------------------------------------------------------------+----------------+-------------------|\n","|  0 | ['rotating', 'beers', 'tap', 'also', 'highlight', 'place']  |              1 |                 0 |\n","|  1 | ['host', 'staff', 'lack', 'better', 'word', 'bitches']      |              0 |                 1 |\n","|  2 | ['pizza', 'tasted', 'old', 'super', 'chewy', 'good', 'way'] |              0 |                 1 |\n","|  3 | ['attached', 'gas', 'station', 'rarely', 'good', 'sign']    |              0 |                 1 |\n","+----+-------------------------------------------------------------+----------------+-------------------+\n"],"name":"stdout"}]}]}